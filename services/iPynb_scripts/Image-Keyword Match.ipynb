{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-23 16:36:55,189 : INFO : loading Word2Vec object from ../trained_model/model\n",
      "2018-02-23 16:36:58,396 : INFO : loading vocabulary recursively from ../trained_model/model.vocabulary.* with mmap=None\n",
      "2018-02-23 16:36:58,401 : INFO : loading wv recursively from ../trained_model/model.wv.* with mmap=None\n",
      "2018-02-23 16:36:58,405 : INFO : loading vectors from ../trained_model/model.wv.vectors.npy with mmap=None\n",
      "2018-02-23 16:36:58,605 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-02-23 16:36:58,606 : INFO : loading trainables recursively from ../trained_model/model.trainables.* with mmap=None\n",
      "2018-02-23 16:36:58,607 : INFO : loading syn1neg from ../trained_model/model.trainables.syn1neg.npy with mmap=None\n",
      "2018-02-23 16:36:58,962 : INFO : setting ignored attribute cum_table to None\n",
      "2018-02-23 16:36:58,962 : INFO : loaded ../trained_model/model\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:43: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['addiction', 'advice', 'announcement message', 'care', 'clinic', 'colour', 'communication', 'concepts', 'consultant', 'danger', 'doctor', 'expertise', 'forbidden', 'general practitioner', 'healthcare and medicine', 'healthcare worker', 'healthy lifestyle', 'holding', 'horizontal', 'hospital', 'isolated', 'isolated on white', 'lab coat', 'male', 'man', 'medical occupation', 'medical test', 'medicine', 'message', 'no smoking sign', 'occupation', 'people', 'photography', 'professional occupation', 'showing', 'sign', 'smoking', 'smoking issues', 'social issues', 'stethoscope', 'stop', 'teaching', 'uniform'] 1.0\n"
     ]
    }
   ],
   "source": [
    "import pyexiv2\n",
    "import nltk\n",
    "import os\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sematch.semantic.similarity import WordNetSimilarity\n",
    "import gensim, logging, os\n",
    "import PIL.Image as Image\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# keywords =list(map(stemmer.stem, ['ache','understand']))\n",
    "keywords=['smoking']\n",
    "def getDictOfKeywords(dir):\n",
    "    dictOfimg = {}\n",
    "    for file in os.listdir(dir):\n",
    "        if file[0] == \".\":\n",
    "            continue\n",
    "        metadata = pyexiv2.ImageMetadata(dir + '//' + file)\n",
    "        metadata.read()\n",
    "        if('Iptc.Application2.Keywords' in metadata.iptc_keys):\n",
    "            dictOfimg[file] = metadata['Iptc.Application2.Keywords'].raw_value\n",
    "        elif 'Xmp.MicrosoftPhoto.LastKeywordXMP' in metadata.keys():\n",
    "            dictOfimg[file] = metadata['Xmp.MicrosoftPhoto.LastKeywordXMP'].raw_value\n",
    "        else:\n",
    "            dictOfimg[file] = []\n",
    "#         dictOfimg[file] = list(map(stemmer.stem, dictOfimg[file]))\n",
    "    return dictOfimg\n",
    "\n",
    "    \n",
    "def getSimilarImage1():\n",
    "    images = getDictOfKeywords('..//..//StoryCareImages')\n",
    "    maxSim = 0\n",
    "    imageSim = ''\n",
    "    wns = WordNetSimilarity()\n",
    "    model = gensim.models.Word2Vec.load('../trained_model/model')\n",
    "\n",
    "    for image in images.keys():\n",
    "        avg = 0\n",
    "#         print(images[image])\n",
    "        for k in keywords:\n",
    "            matches=[]\n",
    "            for word in images[image]:\n",
    "                if ' ' not in word and k in model.wv and word in model.wv:\n",
    "                    matches.append(model.similarity(k.lower(), word.lower()))\n",
    "            avg += max(matches) if matches else 0\n",
    "        similarity = avg/float(len(keywords))\n",
    "        if(similarity > maxSim):\n",
    "            maxSim = similarity\n",
    "            imageSim = image\n",
    "    print images[imageSim],maxSim\n",
    "    return imageSim\n",
    "\n",
    "im = Image.open(\"../../StoryCareImages/\" + getSimilarImage1())\n",
    "im.show()\n",
    "\n",
    "#getSimilarImage1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Chlorthalidone' in model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-23 15:20:34,068 : INFO : loading Word2Vec object from trained_model\n",
      "2018-02-23 15:20:38,151 : INFO : loading vocabulary recursively from trained_model.vocabulary.* with mmap=None\n",
      "2018-02-23 15:20:38,152 : INFO : loading wv recursively from trained_model.wv.* with mmap=None\n",
      "2018-02-23 15:20:38,153 : INFO : loading vectors from trained_model.wv.vectors.npy with mmap=None\n",
      "2018-02-23 15:20:38,782 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-02-23 15:20:38,785 : INFO : loading trainables recursively from trained_model.trainables.* with mmap=None\n",
      "2018-02-23 15:20:38,786 : INFO : loading syn1neg from trained_model.trainables.syn1neg.npy with mmap=None\n",
      "2018-02-23 15:20:39,692 : INFO : setting ignored attribute cum_table to None\n",
      "2018-02-23 15:20:39,700 : INFO : loaded trained_model\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging, os\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = gensim.models.Word2Vec.load('trained_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06367981084439678"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['head ache', 'understand']\n",
      "{<type 'file'>: []}\n"
     ]
    }
   ],
   "source": [
    "# dictOfimg = {}\n",
    "# metadata = pyexiv2.ImageMetadata('..//..//StoryCare images/shutterstock_71236504.jpg')\n",
    "# metadata.read()\n",
    "# print(metadata.iptc_keys)\n",
    "# print(metadata['Xmp.MicrosoftPhoto.LastKeywordXMP'].raw_value)\n",
    "# if('Iptc.Application2.Keywords' in metadata.iptc_keys):\n",
    "#     dictOfimg[file] = metadata['Iptc.Application2.Keywords'].raw_value\n",
    "# else:\n",
    "#     dictOfimg[file] = []\n",
    "#     dictOfimg[file] = list(map(stemmer.stem, dictOfimg[file]))\n",
    "# print(dictOfimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wns = WordNetSimilarity()\n",
    "# wns.word_similarity('relax', 'relax', 'wpath') # 0.449327301063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('remainder.n.01.remainder'), Lemma('remainder.n.01.balance'), Lemma('remainder.n.01.residual'), Lemma('remainder.n.01.residue'), Lemma('remainder.n.01.residuum'), Lemma('remainder.n.01.rest')]\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import wordnet\n",
    "# syns = wordnet.synsets('rest')\n",
    "# print(syns[0].lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'rest']\n"
     ]
    }
   ],
   "source": [
    "#  for syn in wordnet.synsets(\"rest\"):\n",
    "#     synonyms = []\n",
    "#     w1 = syn\n",
    "#     w2 \n",
    "#     w1.wup_similarity(w2)\n",
    "#     for l in syn.lemmas():\n",
    "#         synonyms.append(l.name())\n",
    "# print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.285714285714\n",
      "[Synset('remainder.n.01'), Synset('rest.n.02'), Synset('respite.n.04'), Synset('rest.n.04'), Synset('rest.n.05'), Synset('rest.n.06'), Synset('rest.n.07'), Synset('rest.v.01'), Synset('rest.v.02'), Synset('rest.v.03'), Synset('lie.v.06'), Synset('rest.v.05'), Synset('stay.v.01'), Synset('rest.v.07'), Synset('rest.v.08'), Synset('perch.v.01'), Synset('pillow.v.01'), Synset('rest.v.11')]\n"
     ]
    }
   ],
   "source": [
    "# w1 = wordnet.synset('ache.v.01') # v here denotes the tag verb\n",
    "# w2 = wordnet.synset('head.v.01')\n",
    "# print(w1.wup_similarity(w2))\n",
    "\n",
    "# print(wordnet.synsets('rest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.470588235294\n"
     ]
    }
   ],
   "source": [
    "# from itertools import product\n",
    "# import numpy as np\n",
    "# allsyns1 = set(ss for word in ['understand','ache'] for ss in wordnet.synsets(word))\n",
    "# allsyns2 = set(ss for word in ['understand','ache'] for ss in wordnet.synsets(word))\n",
    "# best = 0.0\n",
    "# word = allsyns1.pop()\n",
    "# for word in allsyns1:\n",
    "#     max = 0\n",
    "#     for s1 in allsyns2:\n",
    "#         similarity = wordnet.wup_similarity(word, s1)\n",
    "#         if(similarity and similarity>max):\n",
    "#             max = similarity\n",
    "#     best += max\n",
    "# print(best/(len(list(allsyns1)) + len(list(allsyns2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import wordnet\n",
    "\n",
    "# list1 = ['head', 'ache']\n",
    "# list2 = ['head', 'ache'] \n",
    "# list11 = []\n",
    "\n",
    "# for word1 in list1:\n",
    "#     for word2 in list2:\n",
    "#         wordFromList1 = wordnet.synsets(word1)\n",
    "#         wordFromList2 = wordnet.synsets(word2)\n",
    "#         if wordFromList1 and wordFromList2: #Thanks to @alexis' note\n",
    "#             s = wordFromList1[0].wup_similarity(wordFromList2[0])\n",
    "#             if(s):\n",
    "#                 list11.append(s)\n",
    "\n",
    "# print(np.mean(list11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
