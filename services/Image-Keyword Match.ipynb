{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shutterstock_572854942.jpg\n",
      "shutterstock_335916854.jpg\n",
      "shutterstock_572894623.jpg\n",
      "shutterstock_540997561.jpg\n",
      "shutterstock_553662235.jpg\n",
      "shutterstock_596085695.jpg\n",
      "shutterstock_88271590.jpg\n",
      "shutterstock_102840245.jpg\n",
      "shutterstock_759846457.jpg\n",
      "shutterstock_279908831.jpg\n",
      "shutterstock_559105132.jpg\n",
      "shutterstock_113477533.jpg\n",
      "shutterstock_584573320.jpg\n",
      "shutterstock_77409244.jpg\n",
      "shutterstock_196449920.jpg\n",
      "shutterstock_79294636.jpg\n",
      "shutterstock_302044346.jpg\n",
      "shutterstock_667699744.jpg\n",
      "shutterstock_129433802.jpg\n",
      "shutterstock_691644811.jpg\n",
      "shutterstock_256491202.jpg\n",
      "shutterstock_86830399.jpg\n",
      "shutterstock_245873251.jpg\n",
      "shutterstock_538629076.jpg\n",
      "shutterstock_306650666.jpg\n",
      "shutterstock_709643614.jpg\n",
      "shutterstock_121813225.jpg\n",
      "shutterstock_615860714.jpg\n",
      "shutterstock_732175282.jpg\n",
      "shutterstock_540635956.jpg\n",
      "shutterstock_57700177.jpg\n",
      "shutterstock_46714294.jpg\n",
      "shutterstock_115219357.jpg\n",
      "shutterstock_626866760.jpg\n",
      "shutterstock_127206167.jpg\n",
      "shutterstock_281147240.jpg\n",
      "shutterstock_359505965.jpg\n",
      "shutterstock_77003545.jpg\n",
      "shutterstock_683325046.jpg\n",
      "shutterstock_537358684.jpg\n",
      "shutterstock_397865059.jpg\n",
      "shutterstock_262732652.jpg\n",
      "shutterstock_516507907.jpg\n",
      "shutterstock_745443628.jpg\n",
      "shutterstock_742700083.jpg\n",
      "shutterstock_327930536.jpg\n",
      "shutterstock_73707667.jpg\n",
      "shutterstock_732055678.jpg\n",
      "shutterstock_504203647.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'shutterstock_79294636.jpg'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyexiv2\n",
    "import nltk\n",
    "import os\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sematch.semantic.similarity import WordNetSimilarity\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "keywords =list(map(stemmer.stem, ['ache','understand']))\n",
    "def getDictOfKeywords(dir):\n",
    "    dictOfimg = {}\n",
    "    for file in os.listdir(dir):\n",
    "        if file[0] == \".\":\n",
    "            continue\n",
    "        metadata = pyexiv2.metadata.ImageMetadata(dir + '/' + file)\n",
    "        metadata.read()\n",
    "        if('Iptc.Application2.Keywords' in metadata.iptc_keys):\n",
    "            dictOfimg[file] = metadata['Iptc.Application2.Keywords'].raw_value\n",
    "        elif 'Xmp.MicrosoftPhoto.LastKeywordXMP' in metadata.keys():\n",
    "            dictOfimg[file] = metadata['Xmp.MicrosoftPhoto.LastKeywordXMP'].raw_value\n",
    "        else:\n",
    "            dictOfimg[file] = []\n",
    "        dictOfimg[file] = list(map(stemmer.stem, dictOfimg[file]))\n",
    "    return dictOfimg\n",
    "\n",
    "def getSimilarImage():\n",
    "    images = getDictOfKeywords('..//StoryCareImages')\n",
    "    maxSim = 0\n",
    "    imageSim = ''\n",
    "    for image in images.keys():\n",
    "        count = 0\n",
    "        for k in keywords:\n",
    "            if k in images[image]:\n",
    "                count+=1\n",
    "        similarity = count/float(len(keywords))\n",
    "        if(similarity > maxSim):\n",
    "            similarity = maxSim\n",
    "            imageSim = image\n",
    "    return imageSim\n",
    "    \n",
    "def getSimilarImage1():\n",
    "    images = getDictOfKeywords('../StoryCareImages')\n",
    "    maxSim = 0\n",
    "    imageSim = ''\n",
    "    wns = WordNetSimilarity()\n",
    "    for image in images.keys():\n",
    "        avg = 0\n",
    "        print(images[image])\n",
    "        for word in images[image]:\n",
    "            max = 0\n",
    "            for k in keywords:\n",
    "                print(k,word)\n",
    "                match = wns.word_similarity(k, word, 'li')\n",
    "                print(match)\n",
    "                if(max<match):\n",
    "                    max = match\n",
    "            avg += max\n",
    "            print(avg)\n",
    "        similarity = 0\n",
    "        if(len(images[image]) > 0):\n",
    "            similarity = avg/float(len(images[image]))\n",
    "        if(similarity > maxSim):\n",
    "            similarity = maxSim\n",
    "            imageSim = image\n",
    "    return imageSim\n",
    "\n",
    "getSimilarImage()\n",
    "\n",
    "#getSimilarImage1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['head ache', 'understand']\n",
      "{<type 'file'>: []}\n"
     ]
    }
   ],
   "source": [
    "# dictOfimg = {}\n",
    "# metadata = pyexiv2.ImageMetadata('..//..//StoryCare images/shutterstock_71236504.jpg')\n",
    "# metadata.read()\n",
    "# print(metadata.iptc_keys)\n",
    "# print(metadata['Xmp.MicrosoftPhoto.LastKeywordXMP'].raw_value)\n",
    "# if('Iptc.Application2.Keywords' in metadata.iptc_keys):\n",
    "#     dictOfimg[file] = metadata['Iptc.Application2.Keywords'].raw_value\n",
    "# else:\n",
    "#     dictOfimg[file] = []\n",
    "#     dictOfimg[file] = list(map(stemmer.stem, dictOfimg[file]))\n",
    "# print(dictOfimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wns = WordNetSimilarity()\n",
    "# wns.word_similarity('relax', 'relax', 'wpath') # 0.449327301063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('remainder.n.01.remainder'), Lemma('remainder.n.01.balance'), Lemma('remainder.n.01.residual'), Lemma('remainder.n.01.residue'), Lemma('remainder.n.01.residuum'), Lemma('remainder.n.01.rest')]\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import wordnet\n",
    "# syns = wordnet.synsets('rest')\n",
    "# print(syns[0].lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'rest']\n"
     ]
    }
   ],
   "source": [
    "#  for syn in wordnet.synsets(\"rest\"):\n",
    "#     synonyms = []\n",
    "#     w1 = syn\n",
    "#     w2 \n",
    "#     w1.wup_similarity(w2)\n",
    "#     for l in syn.lemmas():\n",
    "#         synonyms.append(l.name())\n",
    "# print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.285714285714\n",
      "[Synset('remainder.n.01'), Synset('rest.n.02'), Synset('respite.n.04'), Synset('rest.n.04'), Synset('rest.n.05'), Synset('rest.n.06'), Synset('rest.n.07'), Synset('rest.v.01'), Synset('rest.v.02'), Synset('rest.v.03'), Synset('lie.v.06'), Synset('rest.v.05'), Synset('stay.v.01'), Synset('rest.v.07'), Synset('rest.v.08'), Synset('perch.v.01'), Synset('pillow.v.01'), Synset('rest.v.11')]\n"
     ]
    }
   ],
   "source": [
    "# w1 = wordnet.synset('ache.v.01') # v here denotes the tag verb\n",
    "# w2 = wordnet.synset('head.v.01')\n",
    "# print(w1.wup_similarity(w2))\n",
    "\n",
    "# print(wordnet.synsets('rest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.470588235294\n"
     ]
    }
   ],
   "source": [
    "# from itertools import product\n",
    "# import numpy as np\n",
    "# allsyns1 = set(ss for word in ['understand','ache'] for ss in wordnet.synsets(word))\n",
    "# allsyns2 = set(ss for word in ['understand','ache'] for ss in wordnet.synsets(word))\n",
    "# best = 0.0\n",
    "# word = allsyns1.pop()\n",
    "# for word in allsyns1:\n",
    "#     max = 0\n",
    "#     for s1 in allsyns2:\n",
    "#         similarity = wordnet.wup_similarity(word, s1)\n",
    "#         if(similarity and similarity>max):\n",
    "#             max = similarity\n",
    "#     best += max\n",
    "# print(best/(len(list(allsyns1)) + len(list(allsyns2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import wordnet\n",
    "\n",
    "# list1 = ['head', 'ache']\n",
    "# list2 = ['head', 'ache'] \n",
    "# list11 = []\n",
    "\n",
    "# for word1 in list1:\n",
    "#     for word2 in list2:\n",
    "#         wordFromList1 = wordnet.synsets(word1)\n",
    "#         wordFromList2 = wordnet.synsets(word2)\n",
    "#         if wordFromList1 and wordFromList2: #Thanks to @alexis' note\n",
    "#             s = wordFromList1[0].wup_similarity(wordFromList2[0])\n",
    "#             if(s):\n",
    "#                 list11.append(s)\n",
    "\n",
    "# print(np.mean(list11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
