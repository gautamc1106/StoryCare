{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shutterstock_71236504.jpg'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyexiv2\n",
    "import nltk\n",
    "import os\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sematch.semantic.similarity import WordNetSimilarity\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "keywords =list(map(stemmer.stem, ['ache','understand']))\n",
    "def getDictOfKeywords(dir):\n",
    "    dictOfimg = {}\n",
    "    for file in os.listdir(dir):\n",
    "        if file[0] == \".\":\n",
    "            continue\n",
    "        metadata = pyexiv2.ImageMetadata(dir + '//' + file)\n",
    "        metadata.read()\n",
    "        if('Iptc.Application2.Keywords' in metadata.iptc_keys):\n",
    "            dictOfimg[file] = metadata['Iptc.Application2.Keywords'].raw_value\n",
    "        elif 'Xmp.MicrosoftPhoto.LastKeywordXMP' in metadata.keys():\n",
    "            dictOfimg[file] = metadata['Xmp.MicrosoftPhoto.LastKeywordXMP'].raw_value\n",
    "        else:\n",
    "            dictOfimg[file] = []\n",
    "        dictOfimg[file] = list(map(stemmer.stem, dictOfimg[file]))\n",
    "    return dictOfimg\n",
    "\n",
    "def getSimilarImage():\n",
    "    images = getDictOfKeywords('..//..//StoryCare images')\n",
    "    maxSim = 0\n",
    "    imageSim = ''\n",
    "    for image in images.keys():\n",
    "        count = 0\n",
    "        for k in keywords:\n",
    "            if k in images[image]:\n",
    "                count+=1\n",
    "        similarity = count/float(len(keywords))\n",
    "        if(similarity > maxSim):\n",
    "            similarity = maxSim\n",
    "            imageSim = image\n",
    "    return imageSim\n",
    "    \n",
    "def getSimilarImage1():\n",
    "    images = getDictOfKeywords('..//..//StoryCare images')\n",
    "    maxSim = 0\n",
    "    imageSim = ''\n",
    "    wns = WordNetSimilarity()\n",
    "    for image in images.keys():\n",
    "        avg = 0\n",
    "        print(images[image])\n",
    "        for word in images[image]:\n",
    "            max = 0\n",
    "            for k in keywords:\n",
    "                print(k,word)\n",
    "                match = wns.word_similarity(k, word, 'li')\n",
    "                print(match)\n",
    "                if(max<match):\n",
    "                    max = match\n",
    "            avg += max\n",
    "            print(avg)\n",
    "        similarity = 0\n",
    "        if(len(images[image]) > 0):\n",
    "            similarity = avg/float(len(images[image]))\n",
    "        if(similarity > maxSim):\n",
    "            similarity = maxSim\n",
    "            imageSim = image\n",
    "    return imageSim\n",
    "\n",
    "getSimilarImage()\n",
    "\n",
    "#getSimilarImage1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['head ache', 'understand']\n",
      "{<type 'file'>: []}\n"
     ]
    }
   ],
   "source": [
    "dictOfimg = {}\n",
    "metadata = pyexiv2.ImageMetadata('..//..//StoryCare images/shutterstock_71236504.jpg')\n",
    "metadata.read()\n",
    "print(metadata.iptc_keys)\n",
    "print(metadata['Xmp.MicrosoftPhoto.LastKeywordXMP'].raw_value)\n",
    "if('Iptc.Application2.Keywords' in metadata.iptc_keys):\n",
    "    dictOfimg[file] = metadata['Iptc.Application2.Keywords'].raw_value\n",
    "else:\n",
    "    dictOfimg[file] = []\n",
    "    dictOfimg[file] = list(map(stemmer.stem, dictOfimg[file]))\n",
    "print(dictOfimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wns = WordNetSimilarity()\n",
    "wns.word_similarity('relax', 'relax', 'wpath') # 0.449327301063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('remainder.n.01.remainder'), Lemma('remainder.n.01.balance'), Lemma('remainder.n.01.residual'), Lemma('remainder.n.01.residue'), Lemma('remainder.n.01.residuum'), Lemma('remainder.n.01.rest')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets('rest')\n",
    "print(syns[0].lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'rest']\n"
     ]
    }
   ],
   "source": [
    " for syn in wordnet.synsets(\"rest\"):\n",
    "    synonyms = []\n",
    "    w1 = syn\n",
    "    w2 \n",
    "    w1.wup_similarity(w2)\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.285714285714\n",
      "[Synset('remainder.n.01'), Synset('rest.n.02'), Synset('respite.n.04'), Synset('rest.n.04'), Synset('rest.n.05'), Synset('rest.n.06'), Synset('rest.n.07'), Synset('rest.v.01'), Synset('rest.v.02'), Synset('rest.v.03'), Synset('lie.v.06'), Synset('rest.v.05'), Synset('stay.v.01'), Synset('rest.v.07'), Synset('rest.v.08'), Synset('perch.v.01'), Synset('pillow.v.01'), Synset('rest.v.11')]\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ache.v.01') # v here denotes the tag verb\n",
    "w2 = wordnet.synset('head.v.01')\n",
    "print(w1.wup_similarity(w2))\n",
    "\n",
    "print(wordnet.synsets('rest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.470588235294\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "allsyns1 = set(ss for word in ['understand','ache'] for ss in wordnet.synsets(word))\n",
    "allsyns2 = set(ss for word in ['understand','ache'] for ss in wordnet.synsets(word))\n",
    "best = 0.0\n",
    "word = allsyns1.pop()\n",
    "for word in allsyns1:\n",
    "    max = 0\n",
    "    for s1 in allsyns2:\n",
    "        similarity = wordnet.wup_similarity(word, s1)\n",
    "        if(similarity and similarity>max):\n",
    "            max = similarity\n",
    "    best += max\n",
    "print(best/(len(list(allsyns1)) + len(list(allsyns2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "list1 = ['head', 'ache']\n",
    "list2 = ['head', 'ache'] \n",
    "list11 = []\n",
    "\n",
    "for word1 in list1:\n",
    "    for word2 in list2:\n",
    "        wordFromList1 = wordnet.synsets(word1)\n",
    "        wordFromList2 = wordnet.synsets(word2)\n",
    "        if wordFromList1 and wordFromList2: #Thanks to @alexis' note\n",
    "            s = wordFromList1[0].wup_similarity(wordFromList2[0])\n",
    "            if(s):\n",
    "                list11.append(s)\n",
    "\n",
    "print(np.mean(list11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
